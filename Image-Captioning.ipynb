{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import spacy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIRECTORY = \"dataset\"\n",
    "# BASE_DIRECTORY = \"/kaggle/input/flickr8k\"\n",
    "\n",
    "df = pd.read_csv(BASE_DIRECTORY + \"/captions.txt\")\n",
    "df[\"path\"] = df[\"image\"].apply(lambda x: BASE_DIRECTORY + \"/Images/\" + x)\n",
    "df = df.rename(columns={\"image\": \"id\"})\n",
    "df[\"id\"] = df[\"id\"].str.replace(\".jpg\", \"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataframe shape:\", df.shape)\n",
    "unique_id_count = df[\"id\"].nunique()\n",
    "print(\"Number of samples\", unique_id_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for i in range(0, len(df), 5):\n",
    "    id = df[\"id\"][i]\n",
    "    captions = [df[\"caption\"][j] for j in range(i, i + 5)]\n",
    "    path = df[\"path\"][i]\n",
    "    data[id] = {\"captions\": captions, \"path\": path}\n",
    "\n",
    "keys = list(data.keys())\n",
    "\n",
    "key = keys[0]\n",
    "value = data[key]\n",
    "\n",
    "print(f\"Key: {key}\")\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dictionary into train, test, and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ids_from_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        ids = [line.strip() for line in f]\n",
    "    return ids\n",
    "\n",
    "\n",
    "# BASE_DIRECTORY = \"/kaggle/input/id-files\"\n",
    "\n",
    "train_keys = read_ids_from_file(BASE_DIRECTORY + \"/train_id.txt\")\n",
    "val_keys = read_ids_from_file(BASE_DIRECTORY + \"/val_id.txt\")\n",
    "test_keys = read_ids_from_file(BASE_DIRECTORY + \"/test_id.txt\")\n",
    "\n",
    "# Create the training, validation, and testing sets\n",
    "train_data = {key: data[key] for key in train_keys}\n",
    "val_data = {key: data[key] for key in val_keys}\n",
    "test_data = {key: data[key] for key in test_keys}\n",
    "\n",
    "train_keys = list(train_data.keys())\n",
    "val_keys = list(val_data.keys())\n",
    "test_keys = list(test_data.keys())\n",
    "\n",
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(val_data))\n",
    "print(\"Testing set size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Show Image Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    return Image.open(path)\n",
    "\n",
    "\n",
    "def show_image(image):\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def read_from_tensor(tensor):\n",
    "    img_numpy = tensor.permute(1, 2, 0).numpy()\n",
    "    img_numpy = np.clip(img_numpy, 0, 1)\n",
    "    plt.imshow(img_numpy)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of an image with captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "show_image(read_image(train_data[train_keys[index]][\"path\"]))\n",
    "for i in range(5):\n",
    "    print(train_data[train_keys[index]][\"captions\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(\n",
    "            224, scale=(0.9, 1.0), ratio=(0.95, 1.05), antialias=True\n",
    "        ),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data to Tensor Conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = [\n",
    "    train_transform(read_image(train_data[key][\"path\"])) for key in train_keys\n",
    "]\n",
    "train_tensors = torch.stack(train_tensors)\n",
    "\n",
    "val_tensors = [\n",
    "    val_test_transform(read_image(val_data[key][\"path\"])) for key in val_keys\n",
    "]\n",
    "val_tensors = torch.stack(val_tensors)\n",
    "\n",
    "test_tensors = [\n",
    "    val_test_transform(read_image(test_data[key][\"path\"])) for key in test_keys\n",
    "]\n",
    "test_tensors = torch.stack(test_tensors)\n",
    "\n",
    "\n",
    "print(\"Training tensor shape:\", train_tensors.shape)\n",
    "print(\"Validation tensor shape:\", val_tensors.shape)\n",
    "print(\"Testing tensor shape:\", test_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the captions to the image from tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_tensor(train_tensors[index])\n",
    "for i in range(5):\n",
    "    print(train_data[train_keys[index]][\"captions\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, data, keys, tensors):\n",
    "        self.data = data\n",
    "        self.keys = keys\n",
    "        self.tensors = tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        key = self.keys[index]\n",
    "        tensor = self.tensors[index]\n",
    "        captions = self.data[key][\"captions\"]\n",
    "        return tensor, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptionDataset(train_data, train_keys, train_tensors)\n",
    "val_dataset = ImageCaptionDataset(val_data, val_keys, val_tensors)\n",
    "test_dataset = ImageCaptionDataset(test_data, test_keys, test_tensors)\n",
    "\n",
    "X, y = train_dataset[index]\n",
    "print(X)\n",
    "read_from_tensor(X)\n",
    "for i in range(5):\n",
    "    print(y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048 * 7 * 7, 1024)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.relu1(self.fc1(features))\n",
    "        features = self.relu2(self.fc2(features))\n",
    "        print(features.shape)\n",
    "        return features\n",
    "\n",
    "\n",
    "encoder = EncoderCNN().to(device)\n",
    "# torch.save(encoder, 'encoder_model.pth')\n",
    "summary(encoder, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.vocab_map = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.index_map = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.max_length = 50\n",
    "        self.sequences = []\n",
    "\n",
    "    def add_sequence(self, sequence):\n",
    "        self.sequences.append(sequence)\n",
    "\n",
    "    def build_vocab(self, train_data):\n",
    "        for data in train_data:\n",
    "            _, y = data\n",
    "            for sequence in y:\n",
    "                self.add_sequence(sequence)\n",
    "        index = 4\n",
    "        for sequence in self.sequences:\n",
    "            for word in self.tokenize(sequence):\n",
    "                if word not in self.vocab_map:\n",
    "                    self.index_map[index] = word\n",
    "                    self.vocab_map[word] = index\n",
    "                    index += 1\n",
    "\n",
    "    def tokenize(self, sequence):\n",
    "        tokenized_sequence = [token.text.lower() for token in nlp.tokenizer(sequence)]\n",
    "        return tokenized_sequence\n",
    "\n",
    "    def add_padd(self, sequence):\n",
    "\n",
    "        sequence = sequence + [self.vocab_map[\"<PAD>\"]] * (\n",
    "            self.max_length - len(sequence)\n",
    "        )\n",
    "        return sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_map)\n",
    "\n",
    "    def vectorize(self, sequence):\n",
    "        tokenized_sequence = self.tokenize(sequence)\n",
    "        vectorized_sequence = [\n",
    "            (\n",
    "                self.vocab_map[token]\n",
    "                if token in self.vocab_map\n",
    "                else self.vocab_map[\"<UNK>\"]\n",
    "            )\n",
    "            for token in tokenized_sequence\n",
    "        ]\n",
    "        return vectorized_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)\n",
    "        att2 = self.decoder_att(decoder_hidden)\n",
    "        print(att1.shape, att2.shape)\n",
    "        att = self.full_att(self.tanh(att1 + att2.unsqueeze(0))).squeeze(2)\n",
    "        alpha = torch.functional.F.softmax(att, dim=1)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embed_size, hidden_size, num_layers, vocab_size, attention_dim\n",
    "    ):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.attention = Attention(\n",
    "            encoder_dim=input_size, decoder_dim=hidden_size, attention_dim=attention_dim\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embedding(captions)\n",
    "        hidden = torch.zeros(self.num_layers, features.size(0), self.hidden_size).to(\n",
    "            features.device\n",
    "        )\n",
    "        cell = torch.zeros(self.num_layers, features.size(0), self.hidden_size).to(\n",
    "            features.device\n",
    "        )\n",
    "        outputs = torch.zeros(\n",
    "            embeddings.size(0), embeddings.size(1), self.fc.out_features\n",
    "        ).to(features.device)\n",
    "\n",
    "        for t in range(embeddings.size(1) - 1):\n",
    "            attention_weighted_encoding, _ = self.attention(features, hidden)\n",
    "            lstm_input = torch.cat(\n",
    "                (embeddings[:, t], attention_weighted_encoding), dim=1\n",
    "            )\n",
    "            _, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            outputs[:, t, :] = self.fc(hidden)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "hidden_size = 512\n",
    "vocab_size = 100\n",
    "encoder_dim = 2048\n",
    "\n",
    "attention_dim = 256\n",
    "\n",
    "batch_size = 2\n",
    "num_pixels = 1\n",
    "max_caption_length = 6\n",
    "\n",
    "# Randomly generated input data\n",
    "features = torch.randn(batch_size, num_pixels, encoder_dim).to(device)\n",
    "captions = torch.randint(0, vocab_size, (batch_size, max_caption_length)).to(device)\n",
    "\n",
    "# flatten features\n",
    "features = features.view(batch_size, num_pixels, -1)\n",
    "print(features.shape)\n",
    "# Initialize the model\n",
    "decoder = DecoderRNN(\n",
    "    encoder_dim,\n",
    "    embed_size,\n",
    "    hidden_size,\n",
    "    num_layers=1,\n",
    "    vocab_size=vocab_size,\n",
    "    attention_dim=attention_dim,\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "outputs = decoder(features, captions)\n",
    "\n",
    "print(\n",
    "    \"Output shape:\", outputs.shape\n",
    ")  # Expected: (batch_size, max_caption_length, vocab_size)\n",
    "print(\"Outputs:\", outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
