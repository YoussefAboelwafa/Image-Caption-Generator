{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import spacy\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIRECTORY = \"dataset\"\n",
    "# BASE_DIRECTORY = \"/kaggle/input/flickr8k\"\n",
    "\n",
    "df = pd.read_csv(BASE_DIRECTORY + \"/captions.txt\")\n",
    "df[\"path\"] = df[\"image\"].apply(lambda x: BASE_DIRECTORY + \"/Images/\" + x)\n",
    "df = df.rename(columns={\"image\": \"id\"})\n",
    "df[\"id\"] = df[\"id\"].str.replace(\".jpg\", \"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataframe shape:\", df.shape)\n",
    "unique_id_count = df[\"id\"].nunique()\n",
    "print(\"Number of samples\", unique_id_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for i in range(0, len(df), 5):\n",
    "    id = df[\"id\"][i]\n",
    "    captions = [df[\"caption\"][j] for j in range(i, i + 5)]\n",
    "    path = df[\"path\"][i]\n",
    "    data[id] = {\"captions\": captions, \"path\": path}\n",
    "\n",
    "keys = list(data.keys())\n",
    "\n",
    "key = keys[0]\n",
    "value = data[key]\n",
    "\n",
    "print(f\"Key: {key}\")\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dictionary into train, test, and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ids_from_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        ids = [line.strip() for line in f]\n",
    "    return ids\n",
    "\n",
    "\n",
    "# BASE_DIRECTORY = \"/kaggle/input/id-files\"\n",
    "\n",
    "train_keys = read_ids_from_file(BASE_DIRECTORY + \"/train_id.txt\")\n",
    "val_keys = read_ids_from_file(BASE_DIRECTORY + \"/val_id.txt\")\n",
    "test_keys = read_ids_from_file(BASE_DIRECTORY + \"/test_id.txt\")\n",
    "\n",
    "# Create the training, validation, and testing sets\n",
    "train_data = {key: data[key] for key in train_keys}\n",
    "val_data = {key: data[key] for key in val_keys}\n",
    "test_data = {key: data[key] for key in test_keys}\n",
    "\n",
    "train_keys = list(train_data.keys())\n",
    "val_keys = list(val_data.keys())\n",
    "test_keys = list(test_data.keys())\n",
    "\n",
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(val_data))\n",
    "print(\"Testing set size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Show Image Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    return Image.open(path)\n",
    "\n",
    "\n",
    "def show_image(image):\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def read_from_tensor(tensor):\n",
    "    img_numpy = tensor.permute(1, 2, 0).numpy()\n",
    "    img_numpy = np.clip(img_numpy, 0, 1)\n",
    "    plt.imshow(img_numpy)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of an image with captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "show_image(read_image(train_data[train_keys[index]][\"path\"]))\n",
    "for i in range(5):\n",
    "    print(train_data[train_keys[index]][\"captions\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(\n",
    "            224, scale=(0.9, 1.0), ratio=(0.95, 1.05), antialias=True\n",
    "        ),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data to Tensor Conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = [\n",
    "    train_transform(read_image(train_data[key][\"path\"])) for key in train_keys\n",
    "]\n",
    "train_tensors = torch.stack(train_tensors)\n",
    "\n",
    "val_tensors = [\n",
    "    val_test_transform(read_image(val_data[key][\"path\"])) for key in val_keys\n",
    "]\n",
    "val_tensors = torch.stack(val_tensors)\n",
    "\n",
    "test_tensors = [\n",
    "    val_test_transform(read_image(test_data[key][\"path\"])) for key in test_keys\n",
    "]\n",
    "test_tensors = torch.stack(test_tensors)\n",
    "\n",
    "\n",
    "print(\"Training tensor shape:\", train_tensors.shape)\n",
    "print(\"Validation tensor shape:\", val_tensors.shape)\n",
    "print(\"Testing tensor shape:\", test_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the captions to the image from tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_tensor(train_tensors[index])\n",
    "for i in range(5):\n",
    "    print(train_data[train_keys[index]][\"captions\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, data, keys, tensors):\n",
    "        self.data = data\n",
    "        self.keys = keys\n",
    "        self.tensors = tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        key = self.keys[index]\n",
    "        tensor = self.tensors[index]\n",
    "        captions = self.data[key][\"captions\"]\n",
    "        return tensor, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptionDataset(train_data, train_keys, train_tensors)\n",
    "val_dataset = ImageCaptionDataset(val_data, val_keys, val_tensors)\n",
    "test_dataset = ImageCaptionDataset(test_data, test_keys, test_tensors)\n",
    "\n",
    "X, y = train_dataset[index]\n",
    "print(X)\n",
    "read_from_tensor(X)\n",
    "for i in range(5):\n",
    "    print(y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(resnet.children())[\n",
    "            :-2\n",
    "        ]  # Remove the last two layers (fc and avgpool)\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(\n",
    "            resnet.fc.in_features, embed_size\n",
    "        )  # Resnet fc.in_features is 2048\n",
    "        self.ln = nn.LayerNorm(embed_size)  # Use LayerNorm instead of BatchNorm1d\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = self.avgpool(features)\n",
    "        features = features.view(features.size(0), -1)  # Flatten to (batch_size, 2048)\n",
    "        features = self.fc(features)  # Reduce to (batch_size, embed_size)\n",
    "        features = self.ln(features)\n",
    "        features = features.view(\n",
    "            features.size(0), 1, 1, self.embed_size\n",
    "        )  # Reshape to (batch_size, 1, 1, embed_size)\n",
    "        return features\n",
    "\n",
    "\n",
    "# Instantiate the model and move it to the appropriate device\n",
    "encoder = EncoderCNN(512).to(device)\n",
    "\n",
    "# Test the encoder with a single image\n",
    "test_image = train_tensors[index].unsqueeze(0).to(device)\n",
    "print(test_image.shape)\n",
    "\n",
    "feature = encoder(test_image)\n",
    "print(feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.vocab_map = {\"PAD\": 0, \"SOS\": 1, \"EOS\": 2, \"UNK\": 3}\n",
    "        self.index_map = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n",
    "        self.max_length = 0\n",
    "        self.sequences = []\n",
    "\n",
    "    def build_sequences(self):\n",
    "        for data in train_dataset:\n",
    "            _, y = data\n",
    "            for sequence in y:\n",
    "                self.sequences.append(sequence)\n",
    "                self.max_length = max(self.max_length, len(sequence.split(\" \")))\n",
    "        print(len(self.sequences))\n",
    "\n",
    "    def build_vocab(self):\n",
    "        self.build_sequences()\n",
    "        index = 4\n",
    "        for sequence in self.sequences:\n",
    "            for word in self.tokenize(sequence):\n",
    "                if word not in self.vocab_map:\n",
    "                    self.index_map[index] = word\n",
    "                    self.vocab_map[word] = index\n",
    "                    index += 1\n",
    "\n",
    "    def clean(self, sequence):\n",
    "        # preprocessing steps\n",
    "        # convert to lowercase\n",
    "        sequence = sequence.lower()\n",
    "        # delete digits, special chars, etc.,\n",
    "        sequence = re.sub(\"[^A-Za-z]\", \"\", sequence)\n",
    "        # add start and end tags to the caption\n",
    "        sequence = \"SOS \" + sequence + \" EOS\"\n",
    "        return sequence\n",
    "\n",
    "    def tokenize(self, sequence):\n",
    "        tokenized_sequence = []\n",
    "        tokenized_sequence.append(\"SOS\")\n",
    "        for token in nlp.tokenizer(sequence):\n",
    "            if token.text != \".\":\n",
    "                tokenized_sequence.append(token.text.lower())\n",
    "\n",
    "        return tokenized_sequence\n",
    "\n",
    "    def add_padd(self, sequence):\n",
    "\n",
    "        sequence = sequence + \" PAD\" * (self.max_length - len(sequence.split(\" \")))\n",
    "        return sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_map)\n",
    "\n",
    "    def vectorize(self, sequence):\n",
    "        tokenized_sequence = self.tokenize(sequence)\n",
    "        for i in range(len(tokenized_sequence), self.max_length):\n",
    "            tokenized_sequence.append(\"PAD\")\n",
    "        vectorized_sequence = [\n",
    "            (\n",
    "                self.vocab_map[token]\n",
    "                if token in self.vocab_map\n",
    "                else self.vocab_map[\"UNK\"]\n",
    "            )\n",
    "            for token in tokenized_sequence\n",
    "        ]\n",
    "        return vectorized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab()\n",
    "vocab.build_vocab()\n",
    "\n",
    "vocab_map = vocab.vocab_map\n",
    "vocab_size = len(vocab_map)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)\n",
    "        att2 = self.decoder_att(decoder_hidden)\n",
    "        att = self.full_att(self.tanh(att1 + att2.unsqueeze(1))).squeeze(2)\n",
    "        alpha = self.softmax(att)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        input_size,\n",
    "        vocab_size,\n",
    "        attention_dim,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention = Attention(\n",
    "            encoder_dim=input_size, decoder_dim=hidden_size, attention_dim=attention_dim\n",
    "        )\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.init_h = nn.Linear(input_size, hidden_size)\n",
    "        self.init_c = nn.Linear(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_size + input_size, hidden_size, num_layers, batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embedding(captions)\n",
    "        hidden, cell = self.init_hidden(features)\n",
    "        outputs = torch.zeros(captions.size(0), captions.size(1), self.vocab_size).to(\n",
    "            features.device\n",
    "        )\n",
    "        for t in range(captions.size(1)):\n",
    "            attention_weighted_encoding, _ = self.attention(features, hidden[-1])\n",
    "            lstm_input = torch.cat(\n",
    "                (embeddings[:, t], attention_weighted_encoding), dim=1\n",
    "            ).unsqueeze(1)\n",
    "            lstm_output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            outputs[:, t] = self.fc(lstm_output.squeeze(1))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def init_hidden(self, features):\n",
    "        mean_features = features.mean(dim=1)\n",
    "        h = self.init_h(mean_features).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        c = self.init_c(mean_features).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        return h, c\n",
    "\n",
    "    def generate_caption(self, features, vocab, max_caption_length=20):\n",
    "        caption = []\n",
    "        hidden, cell = self.init_hidden(features)\n",
    "        sos = torch.tensor(vocab.vocab_map[\"SOS\"]).view(1, -1).to(features.device)\n",
    "        embed = self.embedding(sos)\n",
    "        for i in range(max_caption_length):\n",
    "            attention_weighted_encoding, _ = self.attention(features, hidden[-1])\n",
    "            print(embed.shape, attention_weighted_encoding.shape)\n",
    "            lstm_input = torch.cat(\n",
    "                (embed[:, 0], attention_weighted_encoding), dim=1\n",
    "            ).unsqueeze(1)\n",
    "            lstm_output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            outputs = self.fc(lstm_output.squeeze(1))\n",
    "            outputs = outputs.argmax(1)\n",
    "            caption.append(outputs.item())\n",
    "            if outputs.item() == vocab.vocab_map[\"EOS\"]:\n",
    "                break\n",
    "            embed = self.embedding(outputs.unsqueeze(0))\n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        input_size,\n",
    "        vocab_size,\n",
    "        attention_dim,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = RNNModule(\n",
    "            embed_size, hidden_size, input_size, vocab_size, attention_dim, num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        features = features.squeeze(0)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512\n",
    "hidden_size = 512\n",
    "input_size = 512\n",
    "attention_dim = 256\n",
    "num_layers = 1\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(\n",
    "    embed_size, hidden_size, input_size, vocab_size, attention_dim, num_layers\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_map[\"PAD\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (images, captions) in enumerate(train_dataloader.dataset):\n",
    "        captions = [vocab.vectorize(caption) for caption in captions]\n",
    "        captions = torch.tensor(captions).to(device)\n",
    "        images = images.to(device)\n",
    "        if len(images.shape) == 3:\n",
    "            images = images.unsqueeze(0)\n",
    "        outputs = model(images, captions[0].unsqueeze(0))\n",
    "        # compute the loss function over the outputs and first caption from the captions\n",
    "        print()\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions[0].view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % print_every == 0:\n",
    "            print(\"Epoch: {} loss: {:.5f}\".format(epoch, loss.item()))\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                dataiter = iter(train_dataloader.dataset)\n",
    "                img, _ = next(dataiter)\n",
    "                features = model.encoder(\n",
    "                    images\n",
    "                )  # Take the first image and add batch dimension\n",
    "                print(features.shape)\n",
    "\n",
    "                caption_ids = model.decoder.generate_caption(\n",
    "                    features.squeeze(0), vocab=vocab, max_caption_length=20\n",
    "                )\n",
    "                caption = \" \".join([vocab.index_map[idx] for idx in caption_ids])\n",
    "                print(caption)\n",
    "                # show_image(img[0], title=caption)\n",
    "\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNNModule(\n",
    "    embed_size, hidden_size, input_size, vocab_size, attention_dim, num_layers\n",
    ").to(device)\n",
    "features = encoder(test_image)\n",
    "captions = (\n",
    "    torch.tensor(vocab.vectorize(\"SOS a dog is running EOS\")).unsqueeze(0).to(device)\n",
    ")\n",
    "features = features.squeeze(0)\n",
    "print(features.shape)\n",
    "print(captions.shape)\n",
    "\n",
    "outputs = rnn(features, captions)\n",
    "print(outputs.shape)\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
